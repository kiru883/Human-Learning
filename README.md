Это мои заметки/конспекты чтобы не забыть, написаны в легкой форме инфы, будут дополняться/исправляться, ПОЖАЛУЙСТА укажите ошибки, буду очень благодарен!


**Содержание**
  - ResNet[classification]
  - ResNext[classification]
  - SENet / SE block[classification / attention]

**Segmentation**
  - FCN-based models[segmentation]
  - U-net[segmentation]
  
  
  

# ResNet
![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/resnet.PNG)

- **Основная мысль** - res. block(fig. c) вместо того чтоб аппроксимироваться к H(x)("идеальный" feature map), с помощью классических методов, апроксимируемся "остатками". 
Н(х) = F(x) + x, где х это инпут, F(x) это остаток, каждый блок использует пропуск соединения(shortcut), мое интуитивное понятие, ассоциация с град. бустингом, 
там мы тоже генерим ОСТАТКИ градиентов лосса и затем суммируем их с предиктом на начальном предикторе, используем BN+ReLu после conv слоев.
- **Fig. A** - основные архитектуры реснет сетей, конв блоки имеют same паддинг, размеры feature map'ов считались исходя из дефолтного размера входного изобр. - **224х224**
- **Fig. C** - основной блок этой сетки, bottleneck, их  2 вида, первый для сетей до 34 слоев, второй для сеток после 34 слоев, линия сбоку - shortcut(identity mapping). 
В настоящих реализациях после конв. используется пакетная нормализация и активация.
- **Fig. B** - пример 34-й архитектуры, разными цветами обозначены блоки с РАЗНЫМ размером feature map'a, используется также 2 вида shortcut'a, первый - классический, обычная передача инпута, второй(штрих) для изменения размера feature map'a, изменять он его может двумя разными методами: А - непараметр. метод, проходимся пулингом 1х1 с страйдом 2, теперь размер feature map'ов изменился но количество каналов такое же, добавляем(18, 34) нулевые/убираем(>34) feature map'ы. В - параметрический метод, он лучше, проходимся конв. 1х1 с страйдом 2(к примеру на изображении было 64 канала, прошлись 128 ядрами с страйдом 2), кол-во фильтров при этом увеличиличивает(18/34 архит.)/уменьшает(>34 архит.)(см. архитектуру) кол-во каналов на след. блоке. bias в conv слоях ОТКЛЮЧАЕТСЯ

Количество параметров
  ![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/resnetP.PNG)

Ссылки
  - https://arxiv.org/pdf/1512.03385.pdf - ориг
  - https://arxiv.org/pdf/1603.05027.pdf - тут утверждают, что relu лучше применять ток к остаткам
  - https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py - реализация от keras team
  - https://towardsdatascience.com/intuition-behind-residual-neural-networks-fa5d2996b2c7
  - https://neurohive.io/ru/vidy-nejrosetej/resnet-34-50-101/
  
  
  
  
  
  
# ResNext
  ![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/resnext.PNG)
  
  - **Основная мысль** -  в использовании grouped convolution(нескольких трансформаций), это приводит к тому, что веса на фильтрах в слое grouped convolution менее коррелируют, это приводит к **более "разным" на выходе feature map'ам(несущих более разную инфу)**, хотя параметров у модели при этом даже чуть меньше чем у resnet'а, архитектура не меняется по сравнению с ресНетом(размер входного изобр. 224х224, после конволюций используем BN+ReLu), однако меняется топология bottleneck'a(а также bias в conv слоях ОТКЛЮЧАЕТСЯ), в этом есть еще несколько плюсов:
    1. **Не меняется кол-во параметров** на одном bottleneck'е, кол-во параметров на resnet'овском ботлнеке: 256 · 64 + 3 · 3 · 64 · 64 + 64 · 256 ≈ 70k,
    на resneXt'овском - C · (256 · d + 3 · 3 · d · d + d · 256), где С - это параметр его ботлнека(cardinality, кол-во трансформаций)
    2. Возможность распаралелить модель из-за grouped convolution(я с этим не сталкивался)
  - **Fig. A** - Сама архитектура, cлева ResNet, справа ResNext(32х4d, это 32 группы каждая по 4 feature map'а(к такому тензору из 4х карт применяем свертку, подробнее визуализ. в Fig. B(b)))
  - **Fig. B** - Bottleneck модели, они эквивалентны, имеют параметр cardinality, кол-во трансформаций, в реализации использовался С вариант
  - **Fig. C** - Скоры на imagenet-1k
 
 
  Мои какие никакие визуализации:
    ![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/ResNextBlockVisual.PNG)
    ![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/rexnextblock.PNG)
    
  - **Fig. D** - Визуализация почему bottleneck'и на fig. B эквивалентны
  - **Fig. E** - Визуализация обычной конволюции у resnet'a(сверху) и grouped convolution у resneXt'a(снизу)
    
Ссылки
  - https://arxiv.org/pdf/1611.05431.pdf - ориг
  - https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215 - почему и как про grouped convolution
  - https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnext50_32x4d - pytorch'евская имплементация 
  - https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cc5d0adf648e - неплохое введение
  - https://github.com/prlz77/ResNeXt.pytorch/blob/master/models/model.py - реализация






# SENet(SE block)
![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/senet.PNG)

- **Основная мысль** - взвешивать feature map'ы(поэлементное произведение весов и соответствующих карт признаков), таким образом, чтобы выделять нужные карты признаков(большой вес) и уменьшать значения малозначимых карт признаков. 
 _Может быть сколько угодно и разнообразно feature map'ов после любого слоя, но если есть набор карт признаков с наиболее выраженным признаком(большое сред. значение в карте признаков) который(набор) характерен для определенного класса, то SE block будет присваивать наибольший вес картам признаков данного класса и меньший вес всем остальным картам признаков._
  Мой абстрактный пример: имеется набор feature map'ов некой сети после некого слоя, причем сеть обучалась на 2-х различных классах(машина и попугай), пропускаем изобр. попугая через сеть, обнаружилось что среди карт признаков "наличие оперенья", "клюв", "крылья", "наличие колес", "стекла" и "монотонность цвета", карты признаков ответственные за признак "наличие оперенья" и "крылья" имеют наибольшие значения(наибольшие дескрипторы полученые через squeeze часть), т.к. в классификации попугая, карты признаков "стекла", "монотонность цвета" и прочие "непопугайные" карты признаков не помогут, следует уменьшить их значимость, а наличие наибольших дескрипторов у "попугайных" карт признаков говорит SE block'у что обьект - попугай, поэтому SE block определяет наибольшие веса "попугайным" feature map'ам, наименьшие всем остальным "не попугайным".
- **fig. A** - SE block'и для resne(x)t'ов(справа) и для inceptions(слева), там же и указание частей блока
- **fig. B** - сам SE block, на примере se-resnet-50 добавляет 2.5 мил. парам., состоит из squeeze, excitation, scale частей:
  1. Squeeze - нужен для получения описания(дескриптора) каждой карты признаков, в оригинале используется global avg. pooling
  2. Excitation - нужен для выделения весов для карт признаков, r это reduction ratio - гиперпараметр который регулирует кол-во нейронов, по дефолту 16
  3. Scale часть, поэлементное умножение карт признаков на веса
- **fig. C** - Архитектуры, на примере с реснетом

Визуализации весов на разных слоях, последняя цифра - номер слоя
    ![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/senetB.PNG)

Ссылки
  - https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7 - вводное
  - https://blog.paperspace.com/channel-attention-squeeze-and-excitation-networks/ - очень хорошее объяснение
  - https://www.robots.ox.ac.uk/~vgg/publications/2018/Hu18/hu18.pdf - ориг
  - https://github.com/last-one/tools/blob/master/pytorch/SE-ResNeXt/SeResNeXt.py - реализация 
  
  
  
  
  

# FCN-based models[segmentation]
![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/fcnA.PNG)
Задача сегментации представляет собой задачу классификации каждого пикселя исходного изображения. Это FCN(fully convolutional network) моделей, по сути состоит из 2-х частей: **encoder**, **decoder**. При этом для классификации пикселей fully connected часть заменяется conv слой с фильтром 1x1. Примеры fcn-подобных архитектур: U-net, SegNet
  1. **Encoder** используется как экстрактор фич, на последних слоях энкодера карты признаков более абстрактны и сложны [*] _и имеют более большой receptive field, а самое главное feature map имеет меньшую размерность чем исходное изображение, это означает что получив к примеру, "на выходе" карту признаков размерностью 1x4x4 и сделав предсказание по каждому из 16 элементов, мы получим очень нечеткую, неточную сегментацию._ 
   Дополнительно стоит отметить что использование обычной архитектуры без пулинга и паддингом = 1(для сохранения размерности feature map'ов) не эффективно из-за: много ест          памяти(большие карты признаков после каждого слоя), маленькие receptive field(из-за отсутствия пулинга), неэффективно извлекает признаки(существуют более эф. архитектуры), не    устойчив к изменению положения объекта(из-за отс. пулинга)
   
  2. **Decoder** используется для повышения размерности(upsampling) feature map'а до размерности исходного изображения. Для этого    используется 2 подхода: параметрический(деконволюция) и непараметрический(бил. интерполяция и пр.). Feature map'ы получаемые на глубоких слоях имеют меньшую размерность, теряют инфу о расположении объекта/текстуры/паттерна на изображении(визуализация на **fig. A**), поэтому для того, чтобы восстановить размерность feature map до размерности исходного изображения, надо    объед. карты с глубоких слоев(имеющие более абстр. признаки и меньшую размерность) с картами менее глубоких слоев(они несут в себе больше информации о расположении объекта, имеют большую размерность), а это skip connection принцип работы которого на **fig C**.

_Польза от такого объединения представлена на **fig. B**, fcn-32 использует только один upsampling(x32), fcn-16 использует upsampling(x2) для    более "поздней"    карты признаков для поэлементного суммирования с более ранней картой признаков и затем upsampling(x16), fcn-8 использует upsampling(x2) для более "поздней"    карты признаков      для поэлементного суммирования с более ранней картой признаков и затем upsampling(х2), получившийся результат суммируется с еще более ранней картой          признаков(еще больше информации о расположении объекта по сравнению с пред. подходами) и затем upsampling(x8). В конце происходит pixelwise prediction с помощью конволюции с фильтром 1х1_

  3. **Output** После получения карты признаков схожей по размерности с размерностью входного изображения, применяем конволюцию 1х1 ко всем картам признаков(кол-во фильтров, равно кол-ву классов), затем к получившимся картам признаков после конв. 1х1, применяем сигмоиду к каждому элементу каждой карты признаков(по сути таким образом мы можем сформировать список к некому пикселю из N элементов, с вероятностями принадлежности N классов с K пикселю).
  
 
![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/fcnB.PNG)

**fig. D** - пример архитектур FCN32, FCN16, FCN8

Ссылки
  - [*] https://youtu.be/gnwG5agGsJ4 - крутая лекция от Соколова
  - https://neurohive.io/ru/vidy-nejrosetej/u-net-image-segmentation/ - короткий обзор(русс)
  - https://www.gsitechnology.com/Beginners-Guide-to-Segmentation-in-Satellite-Images - неплохое введение
  - https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1 - вводный обзор
  - https://arxiv.org/pdf/1411.4038.pdf - ориг(такой себе)






# U-net

![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/unetA.PNG)

  Модель предназначеная для сегментации(semantic/instance). Принадлежит семейству fcn моделей, состоит из енкодера(как и в FCN, для экстракта признаков и нахождение информации о присутствии объекта(информация "WHAT", подробнее в [*]), может использоваться любой другой feature extractor) и декодера(для восстановления карт признаков к размеру наиболее близкому к размеру входного изображения(информация "WHERE", подробнее в [*])). Его отличие от FCN32/16/8:
  1. U-net стэкует карты признаков вместо поэлементного суммирования карт признаков как в FCN
  2. Несколько операций конволюции к стэкованым картам признаков, по мере уменьшения глубины, уменьшение кол-ва каналов
  3. _При skip connection(copy and crop) обрезает передаваемые feature map'ы, как следствие сегментация не принадлежит всему изображению на входе(к примеру на входе изображение 572х572, на выходе сегментация принадлежит центральной области 388х388)_
- **Архитектура** - **fig. A**, по мере роста глубины(encoder) удваивается кол-во каналов(downsampling); по мере уменьшения глубины(decoder) уменьшается в 2 раза число каналов(upsampling), затем к полученому тензору с картами признаков стекуется обрезанные карты признаков с соостветствующей глубины encoder части, несущие в себе больше информации о локализации объекта(серая стрелка); на выходе к получившимся картам признаков применяем тот же подход что и в FCN, свертка 1х1 с числом фильтров, равным числу классов, к получившимся картам для каждого класса применяем сигмойду(для уточнения см. _3.Output в FCN_).
![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/unetB.PNG)
- **fig. B** - Overlap-tile подход, благодаря которому возможна сегментация крупных изображений, желтая зона - зона для сегментации, синяя - входное изображение. На предсказание в желтой области, влияет область синей, таким образом разрезав изображение на патчи можно наиболее точно и безшовно сегментировать большие изображения. Отсутствующая "синяя" часть входного изображения, дополняется отзеркалированной частью 
- **fig. C** - Одна из техник image augmentation - "elastic deformation"
- **fig. D** - Для случаев когда объекты одного класса могут быть близко расположены друг к другу и сегментироваться как один объект, была предложена техника увеличения весов функции потерь на пикселях, граничащих  между двумя такими объектами. Последнее изображение хорошо демонстрирует вес такой границы.

Ссылки
  -      https://zhuanlan.zhihu.com/p/65398511#:~:text=Compared%20to%20FCN%2D8%2C%20the,operator%20instead%20of%20a%20sum.&text=By%20comparison%2C%20the%20basic%20FCN,in%20its%20up%2Dsampling%20path. - неплохое введение 
  - https://dida.do/blog/semantic-segmentation-of-satellite-images 
  - https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760 
  - https://arxiv.org/pdf/1505.04597.pdf - оригинал
    
# Word2Vec
  ![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/word2vec1.PNG)

  Семейство моделей прездназначеных для выделения word embeddings(векторное представление слов, визуализация - **fig. A**), состоит из 2-х разновидностей моделей skip-gram и CBOW, о них позже. Основная суть word2vec в том, что с векторами становится возможным проводить ариф. операции, можно находить разность, сумму, например: "король" + ("лев" - "львица") ~= "королева", так же стало возможо находить наиболее близкие(кос. расстояние, л2 расстояние) вектора, таким образом можно даже построить сою рекоменд. систему на минималках(?). ИМХО, в оригинале не представленны точные архитектуры моделей(в основном это 3-х слойные нейронки), везде разные реализации, можно запутаться, я собрал некоторые из них.
- **Контекст** - Ближайшие слова к "таргет" слову,близость определяется шириной окна - сколько слов(спереди и сзади) от центрального, "таргет слова", считать словами контекта по отношению к таргет слову, **fig. B** - наглядная визуализация.
- **Skip-gram** - Архитектуры везде разные, основной смысл этой - предсказывать контекст(ближайшие слова) по "таргет" слову, input - OneHotEnc. вектор с таргет словом, который затем домножается на матрицу эмбендингов и матрицу контекста, на выходе получаем вероятность по каждому слову(что слово является контектом, softmax) **fig. C** - визуализация
- **CBOW** - **fig. D**, Основной смысл - предсказывать таргет слово по контекту(нескольим ближайшим словам), на входе oneHotEnc. векторы c словами контекста, каждый домножается на матрицу эмбендингов и усредняется(тут так сказано [3]), получ. вектор домножается на матрицу контекста, применяется softmax(вероятности принадлежности Н-ного слова к таргет слову).

Много вариаций функций ошибок(cross entropy(log loss), NCE, Hierarchical Softmax и т.д), подробнее в [1], я рассмотрел только log loss для skip-gramm и CBOW.

Ссылки
  - https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html [1]
  - https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/
  - https://arxiv.org/pdf/1411.2738.pdf [3]
  - https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281 - одна из реализаций skip-gramm
  - https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html - еще skip-gramm
  
  
  # Seq2seq. LSTM 
  ![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/lstm_new.JPG)
  
  LSTM - Long short term memory, слой позволящий извлекать информацию(hidden state) по элементам последовательности. Например объект - фильм, элементы последовательности - кадры, требуется предсказать поведение персонажей, по одному кадру сделать это нельзя(запредиктим что персонаж стоит, хотя на самом деле - танцует если посмотреть по нескольким кадрам, на что способна LSTM).     
  - **fig. A** - Ячейка LSTM. Ее составляющие: cell state(можно расценивать как "долгую память", интуитивно над этой веткой проводятся только поэлементные операции матриц) и hidden state("короткая память", не поэл. операции). Ее внутрненние составляющие: forget gate(фильтр или гейт забывания, например в начале фильма показали персонажа Б, с тех пор прошло пол фильма и теперь в LSTM отправляется, элемент последовательности, кадр персонаж А, информация в cell state о персонаже Б неуместна, следует "уменьшить" инфу о персонаже Б); input gate(фильтр добавления, служит для занесения в cell state новой информации. Например в кадре появился новые персонаж В, заносим в cell state); output gate(выходной фильтр, выдающий новый hidden state содержащий измененную, необходимую информацию в зависимости от элемента последовательности подаваемого на вход и предыдущих элементов(cell state)). __Для входной последовательности мы "проходимся" по каждому элементу также используя hidden/cell state предыдущих.__ Операции в желтом прямоугольнике - параметрические. Hidden/cell state в самом начале(первый элемент) - нулевые векторы.
  - **fig. B** - Мат часть LSTM'а, [] - concat., *(или X) - elementwise произв.
  - **fig. С** - Или я такой тупой что скорее всего или хуй знает или че... но много где можно увидеть такую визуализацию LSTM'а из-за чего начинает казаться что это многоячеестая структура, но нет, на самом деле все операции в LSTM слое проводятся с одной LSTM ячейкой. Чуть выше можно посмотреть разницу между pytorch LSTM/LSTMCell, ее нету(за исключением входного размера, второй для одного элемента), LSTM слой это оптимизированный цикл из LSTMCell. 
  - **fig. D** - Так же существуют BiLSTM, в этому случае входная последовательность просто реверсится и прогоняется через другой слой LSTM'а, плюсы - возможность посмотреть на последовательность под другой очередностью.

![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/seq2seq.png)
Seq2seq - sequential to sequential модель, позволяет образовывать последовательности с последовательностей, seq2seq справляется с задачей перевода и задачами по типу "последовательность в последовательность", состоит из encoder'а и decoder'а, задача первого образовать вектор контекста, задача второго востановить последовательность по вектору контекста.
  На рисунке пример машинного перевода с помощью сек2секов, конечно вместо LSTM'ов могут быть другие реккурентные слои. В энкодер подаются эмбединги слов, на выходу(с последнего элемента последовательности, или тега <EOS> имеем вектор контекста). В декодер подается context vector и в качестве первого элемента последовательности - тег начала предложения(<SOS>), на выходе с первого LSTM'а получается hidden state и вектор пренадлежности начала предложения к i-му слову с нашего словаря(в нашем случае перевода с русского на англ. наиб. вероятность у 'how'), в последующие ячейки мы будем передавать hidden state предыдущей и предыдущий предсказанный эмбединг(в pytorch такое можно сделать передав в слой Embedding индекс слова, возвращается эмбединг, таким образом нам остается определить индекс наиб. элемента в векторе принадлежности, кстати Embedding слой возвращает не CBOW/skipgram векторы, а векторы которые получаются в ходе обучения).
  
Ссылки
  - https://colah.github.io/posts/2015-08-Understanding-LSTMs/ - введение в lstm
  - https://habr.com/ru/company/wunderfund/blog/331310/ - первая статья, перевод
  - http://blog.echen.me/2017/05/30/exploring-lstms/?imm_mid=0f2ce7&cmp=em-data-na-na-newsltr_20170614 - углубление в LSTM
  - https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html - LSTM в торче
  - https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html - seq2seq торчевская реализация, attention часть смотреть не обязательно, достаточно encoder, decoder, training


  
